{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_horizon = 250\n",
    "n = 10\n",
    "horizons=np.arange(max_horizon, -1, -max_horizon // n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/MsPacman-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/MsPacman-v5', render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Create and wrap the environment\n",
    "env = gym.make('ALE/MsPacman-v5-ram')  # Replace with your specific Pac-Man environment\n",
    "env_name = EnvironmentName(\"ALE/MsPacman-v5\")\n",
    "env_kwargs = {}\n",
    "normalize = False\n",
    "seed = 0\n",
    "frame_stack = 1\n",
    "vec_env_type = \"dummy\"\n",
    "vec_env_class = {\"dummy\": DummyVecEnv, \"subproc\": SubprocVecEnv}[vec_env_type]\n",
    "\n",
    "spec = gym.spec(env_name.gym_id)\n",
    "\n",
    "def make_env(**kwargs) -> gym.Env:\n",
    "    return spec.make(**kwargs)\n",
    "\n",
    "# Make the environment\n",
    "env = make_atari_env(\n",
    "    make_env,\n",
    "    n_envs=8,\n",
    "    seed=seed,\n",
    "    vec_env_cls=vec_env_class,\n",
    "    vec_env_kwargs=env_kwargs,\n",
    ")\n",
    "if frame_stack:\n",
    "    env = VecFrameStack(env, n_stack=frame_stack)\n",
    "\n",
    "if not is_vecenv_wrapped(env, VecTransposeImage):\n",
    "    wrap_with_vectranspose = False\n",
    "    if isinstance(env.observation_space, spaces.Dict):\n",
    "        # If even one of the keys is an image-space in need of transpose, apply transpose\n",
    "        # If the image spaces are not consistent (for instance, one is channel first,\n",
    "        # the other channel last); VecTransposeImage will throw an error\n",
    "        for space in env.observation_space.spaces.values():\n",
    "            wrap_with_vectranspose = wrap_with_vectranspose or (\n",
    "                is_image_space(space) and not is_image_space_channels_first(space)  # type: ignore[arg-type]\n",
    "            )\n",
    "    else:\n",
    "        wrap_with_vectranspose = is_image_space(env.observation_space) and not is_image_space_channels_first(\n",
    "            env.observation_space  # type: ignore[arg-type]\n",
    "        )\n",
    "\n",
    "    if wrap_with_vectranspose:\n",
    "        print(\"Wrapping the env in a VecTransposeImage.\")\n",
    "        env = VecTransposeImage(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['online.0.weight', 'online.0.bias', 'online.2.weight', 'online.2.bias', 'online.4.weight', 'online.4.bias', 'target.0.weight', 'target.0.bias', 'target.2.weight', 'target.2.bias', 'target.4.weight', 'target.4.bias'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "file = torch.load(\"/Users/navyajain/Jumpstart-Pacman/expert5.chkpt\")\n",
    "\n",
    "file['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navyajain/miniconda3/envs/rl_env/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 14.13GB > 1.51GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "model = DQN(\"CnnPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CnnPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): NatureCNN(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): NatureCNN(\n",
       "      (cnn): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Flatten(start_dim=1, end_dim=-1)\n",
       "      )\n",
       "      (linear): Sequential(\n",
       "        (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['online.0.weight', 'online.0.bias', 'online.2.weight', 'online.2.bias', 'online.4.weight', 'online.4.bias', 'target.0.weight', 'target.0.bias', 'target.2.weight', 'target.2.bias', 'target.4.weight', 'target.4.bias'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file[\"model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a VecTransposeImage.\n",
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navyajain/miniconda3/envs/rl_env/lib/python3.10/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/navyajain/miniconda3/envs/rl_env/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/navyajain/miniconda3/envs/rl_env/lib/python3.10/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 195\u001b[0m\n\u001b[1;32m    186\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    187\u001b[0m             log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    188\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m    192\u001b[0m                 best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma2c-pacman\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# test the agent for mean reward\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# # Save the model\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# model.save(\"a2c-pacman\")\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# # Load the model with the custom policy\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# loaded_model = A2C.load(\"a2c-pacman\", policy=ScriptedPacManPolicy)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[72], line 85\u001b[0m, in \u001b[0;36mScriptedPacManPolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m     83\u001b[0m distance_to_ghost \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(agent_x \u001b[38;5;241m-\u001b[39m ghost_x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(agent_y \u001b[38;5;241m-\u001b[39m ghost_y)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Remove unsafe actions\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m distance_to_ghost \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Adjust threshold as needed\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent_x \u001b[38;5;241m==\u001b[39m ghost_x:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ghost_y \u001b[38;5;241m>\u001b[39m agent_y:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C\n",
    "from typing import Tuple, Any, Optional\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env, make_vec_env\n",
    "from stable_baselines3.common.vec_env import (\n",
    "    DummyVecEnv,\n",
    "    SubprocVecEnv,\n",
    "    VecEnv,\n",
    "    VecFrameStack,\n",
    "    VecNormalize,\n",
    "    VecTransposeImage,\n",
    "    is_vecenv_wrapped,\n",
    ")\n",
    "from huggingface_sb3 import EnvironmentName\n",
    "from collections import OrderedDict\n",
    "from stable_baselines3.common.preprocessing import is_image_space, is_image_space_channels_first\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ScriptedPacManPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, observation_space: gym.spaces.Space, action_space: gym.spaces.Space, *args, **kwargs):\n",
    "        # Initialize the base policy\n",
    "        super(ScriptedPacManPolicy, self).__init__(observation_space, action_space, *args, **kwargs,\n",
    "                                                   net_arch=[dict(pi=[64, 64], vf=[64, 64])])\n",
    "\n",
    "    def predict(self, observation: np.ndarray, state=None, episode_start=False, deterministic=False) -> Tuple[np.ndarray, Any]:\n",
    "        \"\"\"\n",
    "        Predict the action for the given observation using a scripted policy.\n",
    "\n",
    "        Parameters:\n",
    "        - observation: The current state observation from the environment.\n",
    "        - state: Optional state information (if any).\n",
    "        - episode_start: Whether the current step is the beginning of a new episode.\n",
    "        - deterministic: Whether the prediction should be deterministic.\n",
    "\n",
    "        Returns:\n",
    "        - action: The chosen action based on the observation.\n",
    "        - state: Any state information (if applicable).\n",
    "        \"\"\"\n",
    "        # observation = observation[0]\n",
    "        # print(observation[2])\n",
    "        print(observation.shape)\n",
    "\n",
    "        observation = observation.reshape([observation.shape[0], observation.shape[2], observation.shape[3]])\n",
    "        # show environment photo\n",
    "        plt.imshow(observation.mean(axis=0))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        agent_x, agent_y = observation[:2]  # Agent's position\n",
    "        # print(agent_x.shape, agent_y.shape)\n",
    "        # print(agent_x, agent_y)\n",
    "        num_ghosts = int(observation.shape[1] - 2) // 2\n",
    "        ghost_positions = observation[3:3 + num_ghosts * 2]\n",
    "        remaining_observations = observation[3 + num_ghosts * 2:]\n",
    "        pellet_positions = remaining_observations[:len(remaining_observations) // 2]\n",
    "        power_pellet_positions = remaining_observations[len(remaining_observations) // 2:]\n",
    "\n",
    "        # Define the possible actions\n",
    "        actions = {\n",
    "            0: 'NOOP',\n",
    "            1: 'UP',\n",
    "            2: 'RIGHT',\n",
    "            3: 'LEFT',\n",
    "            4: 'DOWN',\n",
    "            5: 'UPRIGHT',\n",
    "            6: 'UPLEFT',\n",
    "            7: 'DOWNRIGHT',\n",
    "            8: 'DOWNLEFT'\n",
    "        }\n",
    "        \n",
    "        # Initialize variables for action selection\n",
    "        safe_directions = set(range(9))  # All possible actions are initially safe\n",
    "        \n",
    "        # Avoid ghosts\n",
    "        for i in range(0, len(ghost_positions), 2):\n",
    "            ghost_x, ghost_y = ghost_positions[i], ghost_positions[i + 1]\n",
    "            distance_to_ghost = abs(agent_x - ghost_x) + abs(agent_y - ghost_y)\n",
    "            # Remove unsafe actions\n",
    "            if distance_to_ghost <= 1:  # Adjust threshold as needed\n",
    "                if agent_x == ghost_x:\n",
    "                    if ghost_y > agent_y:\n",
    "                        safe_directions.discard(4)  # DOWN\n",
    "                    else:\n",
    "                        safe_directions.discard(1)  # UP\n",
    "                elif agent_y == ghost_y:\n",
    "                    if ghost_x > agent_x:\n",
    "                        safe_directions.discard(2)  # RIGHT\n",
    "                    else:\n",
    "                        safe_directions.discard(3)  # LEFT\n",
    "\n",
    "        # Move towards the nearest pellet\n",
    "        nearest_pellet_distance = float('inf')\n",
    "        nearest_pellet_direction = None\n",
    "        \n",
    "        for i in range(0, len(pellet_positions), 2):\n",
    "            pellet_x, pellet_y = pellet_positions[i], pellet_positions[i + 1]\n",
    "            distance = abs(agent_x - pellet_x) + abs(agent_y - pellet_y)\n",
    "            \n",
    "            if distance < nearest_pellet_distance:\n",
    "                nearest_pellet_distance = distance\n",
    "                if agent_x < pellet_x:\n",
    "                    if agent_y < pellet_y:\n",
    "                        nearest_pellet_direction = 7  # DOWNRIGHT\n",
    "                    elif agent_y > pellet_y:\n",
    "                        nearest_pellet_direction = 5  # UPRIGHT\n",
    "                    else:\n",
    "                        nearest_pellet_direction = 2  # RIGHT\n",
    "                elif agent_x > pellet_x:\n",
    "                    if agent_y < pellet_y:\n",
    "                        nearest_pellet_direction = 8  # DOWNLEFT\n",
    "                    elif agent_y > pellet_y:\n",
    "                        nearest_pellet_direction = 6  # UPLEFT\n",
    "                    else:\n",
    "                        nearest_pellet_direction = 3  # LEFT\n",
    "                else:\n",
    "                    if agent_y < pellet_y:\n",
    "                        nearest_pellet_direction = 4  # DOWN\n",
    "                    else:\n",
    "                        nearest_pellet_direction = 1  # UP\n",
    "        \n",
    "        # Choose the final action\n",
    "        if nearest_pellet_direction is not None and nearest_pellet_direction in safe_directions:\n",
    "            action = nearest_pellet_direction\n",
    "        else:\n",
    "            # If no clear path to a pellet, choose a random safe direction\n",
    "            action = np.random.choice(list(safe_directions))\n",
    "\n",
    "        # Return the action and state\n",
    "        return np.array([action]), state\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make('ALE/MsPacman-v5')  # Replace with your specific Pac-Man environment\n",
    "env_name = EnvironmentName(\"ALE/MsPacman-v5\")\n",
    "env_kwargs = {}\n",
    "normalize = False\n",
    "seed = 0\n",
    "frame_stack = 1\n",
    "vec_env_type = \"dummy\"\n",
    "vec_env_class = {\"dummy\": DummyVecEnv, \"subproc\": SubprocVecEnv}[vec_env_type]\n",
    "\n",
    "spec = gym.spec(env_name.gym_id)\n",
    "\n",
    "def make_env(**kwargs) -> gym.Env:\n",
    "    return spec.make(**kwargs)\n",
    "\n",
    "# Make the environment\n",
    "env = make_atari_env(\n",
    "    make_env,\n",
    "    n_envs=8,\n",
    "    seed=seed,\n",
    "    vec_env_cls=vec_env_class,\n",
    "    vec_env_kwargs=env_kwargs,\n",
    ")\n",
    "if frame_stack:\n",
    "    env = VecFrameStack(env, n_stack=frame_stack)\n",
    "\n",
    "if not is_vecenv_wrapped(env, VecTransposeImage):\n",
    "    wrap_with_vectranspose = False\n",
    "    if isinstance(env.observation_space, spaces.Dict):\n",
    "        # If even one of the keys is an image-space in need of transpose, apply transpose\n",
    "        # If the image spaces are not consistent (for instance, one is channel first,\n",
    "        # the other channel last); VecTransposeImage will throw an error\n",
    "        for space in env.observation_space.spaces.values():\n",
    "            wrap_with_vectranspose = wrap_with_vectranspose or (\n",
    "                is_image_space(space) and not is_image_space_channels_first(space)  # type: ignore[arg-type]\n",
    "            )\n",
    "    else:\n",
    "        wrap_with_vectranspose = is_image_space(env.observation_space) and not is_image_space_channels_first(\n",
    "            env.observation_space  # type: ignore[arg-type]\n",
    "        )\n",
    "\n",
    "    if wrap_with_vectranspose:\n",
    "        print(\"Wrapping the env in a VecTransposeImage.\")\n",
    "        env = VecTransposeImage(env)\n",
    "\n",
    "# Create an instance of the A2C model using your custom policy\n",
    "model = A2C(ScriptedPacManPolicy, env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10,\n",
    "            log_interval=10,\n",
    "            progress_bar=True,\n",
    "            callback=EvalCallback(\n",
    "                env, \n",
    "                n_eval_episodes=10, \n",
    "                best_model_save_path=\"a2c-pacman\"))\n",
    "\n",
    "# test the agent for mean reward\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1)\n",
    "\n",
    "# # Save the model\n",
    "# model.save(\"a2c-pacman\")\n",
    "\n",
    "# # Load the model with the custom policy\n",
    "# loaded_model = A2C.load(\"a2c-pacman\", policy=ScriptedPacManPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get obs space\n",
    "obs_space = env.observation_space\n",
    "obs_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see env properties\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.reward_range)\n",
    "print(env.metadata)\n",
    "# episodes length\n",
    "print(env.spec.max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import (\n",
    "    DummyVecEnv,\n",
    "    SubprocVecEnv,\n",
    "    VecEnv,\n",
    "    VecFrameStack,\n",
    "    VecNormalize,\n",
    "    VecTransposeImage,\n",
    "    is_vecenv_wrapped,\n",
    ")\n",
    "\n",
    "from huggingface_sb3 import EnvironmentName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = EnvironmentName(\"ALE/MsPacman-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = gym.spec(env_name.gym_id)\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ALE/MsPacman-v5\"\n",
    "env = gym.make(name)\n",
    "env.spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from stable_baselines3.common.env_util import make_atari_env, make_vec_env\n",
    "from stable_baselines3.common.preprocessing import is_image_space, is_image_space_channels_first\n",
    "from gymnasium import spaces\n",
    "\n",
    "env_name = EnvironmentName(\"ALE/MsPacman-v5\")\n",
    "# custom_hyperparams = OrderedDict([('batch_size', 256),\n",
    "#              ('clip_range', 'lin_0.1'),\n",
    "#              ('ent_coef', 0.01),\n",
    "#              ('env_wrapper',\n",
    "#               ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n",
    "#              ('frame_stack', 4),\n",
    "#              ('learning_rate', 'lin_2.5e-4'),\n",
    "#              ('n_envs', 8),\n",
    "#              ('n_epochs', 4),\n",
    "#              ('n_steps', 128),\n",
    "#              ('n_timesteps', 500000.0),\n",
    "#              ('policy', 'CnnPolicy'),\n",
    "#              ('vf_coef', 0.5),\n",
    "#              ('normalize', False)])\n",
    "env_kwargs = {}\n",
    "n_timesteps = 10\n",
    "normalise = False\n",
    "seed = 42\n",
    "frame_stack = 4\n",
    "vec_env_type = \"dummy\"\n",
    "vec_env_class = {\"dummy\": DummyVecEnv, \"subproc\": SubprocVecEnv}[vec_env_type]\n",
    "\n",
    "# Get environment spec\n",
    "spec = gym.spec(env_name.gym_id)\n",
    "\n",
    "def make_env(**kwargs) -> gym.Env:\n",
    "    return spec.make(**kwargs)\n",
    "\n",
    "# Make the environment\n",
    "env = make_atari_env(\n",
    "    make_env,\n",
    "    n_envs=1,\n",
    "    seed=seed,\n",
    "    vec_env_cls=vec_env_class,\n",
    "    vec_env_kwargs=env_kwargs,\n",
    ")\n",
    "\n",
    "if frame_stack:\n",
    "    env = VecFrameStack(env, n_stack=frame_stack)\n",
    "\n",
    "if not is_vecenv_wrapped(env, VecTransposeImage):\n",
    "    wrap_with_vectranspose = False\n",
    "    if isinstance(env.observation_space, spaces.Dict):\n",
    "        # If even one of the keys is an image-space in need of transpose, apply transpose\n",
    "        # If the image spaces are not consistent (for instance, one is channel first,\n",
    "        # the other channel last); VecTransposeImage will throw an error\n",
    "        for space in env.observation_space.spaces.values():\n",
    "            wrap_with_vectranspose = wrap_with_vectranspose or (\n",
    "                is_image_space(space) and not is_image_space_channels_first(space)  # type: ignore[arg-type]\n",
    "            )\n",
    "    else:\n",
    "        wrap_with_vectranspose = is_image_space(env.observation_space) and not is_image_space_channels_first(\n",
    "            env.observation_space  # type: ignore[arg-type]\n",
    "        )\n",
    "\n",
    "    if wrap_with_vectranspose:\n",
    "        print(\"Wrapping the env in a VecTransposeImage.\")\n",
    "        env = VecTransposeImage(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "        \"CnnPolicy\",\n",
    "        env,\n",
    "        learning_rate = 2.5e-4,\n",
    "        batch_size = 256,\n",
    "        clip_range = 0.1,\n",
    "        ent_coef = 0.01,\n",
    "        vf_coef = 0.5,\n",
    "        n_epochs = 4,\n",
    "        n_steps = 128,\n",
    "        verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"/Users/navyajain/Jumpstart-Pacman/ppo/ALE-MsPacman-v5 copy/policy.pth\", map_location=torch.device('cpu'))\n",
    "# remove all keys with guide in them OrderedDict\n",
    "keys = list(model.keys())\n",
    "for key in keys:\n",
    "    if 'guide' in key:\n",
    "        del model[key]\n",
    "\n",
    "# save it back\n",
    "torch.save(model, \"/Users/navyajain/Jumpstart-Pacman/ppo/ALE-MsPacman-v5 copy/policy.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"v5\", 'zip', \"/Users/navyajain/Jumpstart-Pacman/ppo/ALE-MsPacman-v5_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"/Users/navyajain/Jumpstart-Pacman/examples/v5\")\n",
    "\n",
    "model_2 = PPO(\n",
    "        \"CnnPolicy\",\n",
    "        env,\n",
    "        learning_rate = 2.5e-4,\n",
    "        batch_size = 256,\n",
    "        clip_range = 0.1,\n",
    "        ent_coef = 0.01,\n",
    "        vf_coef = 0.5,\n",
    "        n_epochs = 4,\n",
    "        n_steps = 128,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "# share the weights\n",
    "model_2.policy.load_state_dict(model.policy.state_dict())\n",
    "\n",
    "# save the model\n",
    "model_2.save(\"/Users/navyajain/Jumpstart-Pacman/examples/ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Video recording with stable baselines\n",
    "# code thanks to Antonin RAFFIN\n",
    "# will merge into stable baselines release in future release\n",
    "\n",
    "#from stable_baselines.common.vec_env import VecVideoRecorder #test\n",
    "from stable_baselines3.common.vec_env import VecEnvWrapper, DummyVecEnv, VecNormalize, VecFrameStack, SubprocVecEnv\n",
    "\n",
    "class VecVideoRecorder(VecEnvWrapper):\n",
    "\n",
    "    def __init__(self, venv, video_folder, record_video_trigger,\n",
    "                 video_length=200, name_prefix='rl-video'):\n",
    "        \"\"\"\n",
    "        Wraps a VecEnv or VecEnvWrapper object to record rendered image as mp4 video.\n",
    "        It requires ffmpeg or avconv to be installed on the machine.\n",
    "        :param venv: (VecEnv or VecEnvWrapper)\n",
    "        :param video_folder: (str) Where to save videos\n",
    "        :param record_video_trigger: (func) Function that defines when to start recording.\n",
    "                                            The function takes the current number of step,\n",
    "                                            and returns whether we should start recording or not.\n",
    "        :param video_length: (int)  Length of recorded videos\n",
    "        :param name_prefix: (str) Prefix to the video name\n",
    "        \"\"\"\n",
    "\n",
    "        VecEnvWrapper.__init__(self, venv)\n",
    "\n",
    "        self.env = venv\n",
    "        # Temp variable to retrieve metadata\n",
    "        temp_env = venv\n",
    "\n",
    "        # Unwrap to retrieve metadata dict\n",
    "        # that will be used by gym recorder\n",
    "        while isinstance(temp_env, VecNormalize) or isinstance(temp_env, VecFrameStack):\n",
    "            temp_env = temp_env.venv\n",
    "\n",
    "        if isinstance(temp_env, DummyVecEnv) or isinstance(temp_env, SubprocVecEnv):\n",
    "            metadata = temp_env.get_attr('metadata')[0]\n",
    "        else:\n",
    "            metadata = temp_env.metadata\n",
    "\n",
    "        self.env.metadata = metadata\n",
    "\n",
    "        self.record_video_trigger = record_video_trigger\n",
    "        self.video_recorder = None\n",
    "\n",
    "        self.video_folder = os.path.abspath(video_folder)\n",
    "        os.makedirs(self.video_folder, exist_ok=True)\n",
    "\n",
    "        self.name_prefix = name_prefix\n",
    "        self.step_id = 0\n",
    "        self.video_length = video_length\n",
    "\n",
    "        self.recording = False\n",
    "        self.recorded_frames = 0\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.venv.reset()\n",
    "        self.start_video_recorder()\n",
    "        return obs\n",
    "\n",
    "    def start_video_recorder(self):\n",
    "        self.close_video_recorder()\n",
    "\n",
    "        video_name = '{}-step-{}-to-step-{}'.format(self.name_prefix, self.step_id,  self.step_id + self.video_length)\n",
    "        base_path = os.path.join(self.video_folder, video_name)\n",
    "        self.video_recorder = video_recorder.VideoRecorder(\n",
    "                env=self.env,\n",
    "                base_path=base_path,\n",
    "                metadata={'step_id': self.step_id}\n",
    "                )\n",
    "\n",
    "        self.video_recorder.capture_frame()\n",
    "        self.recorded_frames = 1\n",
    "        self.recording = True\n",
    "\n",
    "    def _video_enabled(self):\n",
    "        return self.record_video_trigger(self.step_id)\n",
    "\n",
    "    def step_wait(self):\n",
    "        obs, rews, dones, infos = self.venv.step_wait()\n",
    "\n",
    "        self.step_id += 1\n",
    "        if self.recording:\n",
    "            self.video_recorder.capture_frame()\n",
    "            self.recorded_frames += 1\n",
    "            if self.recorded_frames > self.video_length:\n",
    "                logger.info(\"Saving video to \", self.video_recorder.path)\n",
    "                self.close_video_recorder()\n",
    "        elif self._video_enabled():\n",
    "                self.start_video_recorder()\n",
    "\n",
    "        return obs, rews, dones, infos\n",
    "\n",
    "    def close_video_recorder(self):\n",
    "        if self.recording:\n",
    "            self.video_recorder.close()\n",
    "        self.recording = False\n",
    "        self.recorded_frames = 0\n",
    "\n",
    "    def close(self):\n",
    "        VecEnvWrapper.close(self)\n",
    "        self.close_video_recorder()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "from stable_baselines3.common import logger\n",
    "video_length = 6120 # @param\n",
    "\n",
    "monitor_dir = \"/Users/navyajain/Jumpstart-Pacman/examples/\"\n",
    "\n",
    "obs = env.reset()\n",
    "env_id = 'MsPacmanNoFrameskip-v4'\n",
    "\n",
    "env = VecVideoRecorder(env, \"/Users/navyajain/Jumpstart-Pacman/examples/\",\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
    "                       name_prefix=\"trained-agent-{}\".format(env_id))\n",
    "\n",
    "env.reset()\n",
    "timesteps = np.zeros((env.num_envs), dtype=np.int32)\n",
    "for _ in range(video_length + 1):\n",
    "    action, _states = model_2.predict(obs)\n",
    "    obs, _, _, _ = env.step(action)\n",
    "    print(_)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
